{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f715e7a-181d-4a8f-8150-6bf449cf0c99",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from misc_utils import dataset_filtering\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from collections import Counter\n",
    "import textstat\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from scipy.sparse import hstack, csr_matrix\n",
    "from sklearn.naive_bayes import ComplementNB\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, f1_score, precision_score, recall_score,\n",
    "    confusion_matrix, classification_report, make_scorer)\n",
    "from sklearn.pipeline import Pipeline\n",
    "from itertools import product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9aefedac-cbb1-4b9b-a3c6-a2a2546385d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "git_repo_path = '/Users/hunterworssam/Datascience'\n",
    "gutenberg_repo_path = os.path.join(git_repo_path, 'gutenberg')\n",
    "gutenberg_analysis_repo = os.path.join(git_repo_path, 'gutenberg_corpus_analysis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f0565518-f89d-4505-887e-54c4f753e416",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV files\n",
    "train = pd.read_csv('final_train.csv')\n",
    "test = pd.read_csv('final_test.csv')\n",
    "val = pd.read_csv('final_val.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bd61a8f1-e19d-4789-bd6a-1107654e12f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to apply the word, line and token counts\n",
    "def enrich_dataframe(df):\n",
    "    count_path = os.path.join(gutenberg_repo_path, 'data', 'counts')\n",
    "    text_path = os.path.join(gutenberg_repo_path, 'data', 'text')\n",
    "    token_path = os.path.join(gutenberg_repo_path, 'data', 'tokens')\n",
    "\n",
    "    df['word_count'] = df['id'].apply(lambda pid: dataset_filtering.get_word_count(pid, count_path))\n",
    "    df['unique_word_count'] = df['id'].apply(lambda pid: dataset_filtering.get_unique_word_count(pid, count_path))\n",
    "    df['line_count'] = df['id'].apply(lambda pid: dataset_filtering.get_line_count(pid, text_path))\n",
    "    df['token_count'] = df['id'].apply(lambda pid: dataset_filtering.get_token_count(pid, token_path))\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae658142-8f2a-47a8-8842-f27cef624328",
   "metadata": {},
   "source": [
    "Token count, unique word count and line count were not added because the nltk module was not downloaded properly at the time of the download."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "65c93e63-5dd6-461d-ac80-02f349ae9979",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (1920, 13)\n",
      "Val shape: (240, 14)\n",
      "Test shape: (240, 14)\n"
     ]
    }
   ],
   "source": [
    "# Apply to all datasets\n",
    "train = enrich_dataframe(train)\n",
    "val = enrich_dataframe(val)\n",
    "test = enrich_dataframe(test)\n",
    "\n",
    "# Check shapes\n",
    "print(\"Train shape:\", train.shape)\n",
    "print(\"Val shape:\", val.shape)\n",
    "print(\"Test shape:\", test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1917f343-327c-4c9e-b0aa-a6bfa15688f2",
   "metadata": {},
   "source": [
    "Add text and raw data from gutenberg import to the dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a6adc958-0ad4-4267-b220-b552c492262f",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_folder = os.path.join(gutenberg_repo_path, 'data', 'text')\n",
    "\n",
    "# Function to load text for a given Project Gutenberg ID\n",
    "def load_book_text(pg_id):\n",
    "    filename = f'{pg_id}_text.txt'\n",
    "    filepath = os.path.join(text_folder, filename)\n",
    "    try:\n",
    "        with open(filepath, 'r', encoding='utf-8') as f:\n",
    "            return f.read()\n",
    "    except FileNotFoundError:\n",
    "        return None  # or \"\", depending on preference\n",
    "\n",
    "# Apply the function to each row\n",
    "train['text'] = train['id'].apply(load_book_text)\n",
    "val['text'] = val['id'].apply(load_book_text)\n",
    "test['text'] = test['id'].apply(load_book_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6c3a5ddb-31e3-4a39-95a1-1de9806217a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_folder = os.path.join(gutenberg_repo_path, 'data', 'raw')\n",
    "\n",
    "# Function to load text for a given Project Gutenberg ID\n",
    "def load_book_text_raw(pg_id):\n",
    "    filename = f'{pg_id}_raw.txt'\n",
    "    filepath = os.path.join(raw_folder, filename)\n",
    "    try:\n",
    "        with open(filepath, 'r', encoding='utf-8') as f:\n",
    "            return f.read()\n",
    "    except FileNotFoundError:\n",
    "        return None  # or \"\", depending on preference\n",
    "\n",
    "# Apply the function to each row\n",
    "train['raw'] = train['id'].apply(load_book_text_raw)\n",
    "val['raw'] = val['id'].apply(load_book_text_raw)\n",
    "test['raw'] = test['id'].apply(load_book_text_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f15d22-e5d8-4e9d-837f-ca67351e3e68",
   "metadata": {},
   "source": [
    "Confirm all rows have text data appended."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e81d07da-f151-46eb-802f-a0d5233953ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing raw files: 0\n"
     ]
    }
   ],
   "source": [
    "print(\"Missing raw files:\", train['raw'].isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a16174de-b566-42c3-87ce-808848c9b3a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing text files: 0\n"
     ]
    }
   ],
   "source": [
    "print(\"Missing text files:\", train['text'].isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a497e5aa-62a5-4f33-af9a-b671dd74d4f0",
   "metadata": {},
   "source": [
    "It was noted that the first page of many of these texts includes the author and title which can potentially influence our models. Lats remove the first 100 words from all texts to make sure this is not cofounding our results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9833464e-4ad9-44c5-bd39-614ae84a8953",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_first_50_words(text):\n",
    "    if not isinstance(text, str):\n",
    "        return text  # Leave NaN or None untouched\n",
    "    words = text.split()\n",
    "    return ' '.join(words[50:])  # Remove first 50 words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2bbbb5b6-fb95-45b4-a77d-85550cf7971f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess text column\n",
    "train['text'] = train['text'].apply(remove_first_50_words)\n",
    "val['text'] = val['text'].apply(remove_first_50_words)\n",
    "test['text'] = test['text'].apply(remove_first_50_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c907ff-762e-461b-90fb-ae76893fbafd",
   "metadata": {},
   "source": [
    "#### Texts have been loaded in, now we can begin TF-IDF Vectorization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bb9ffc0f-1393-4b54-93c2-89c07640ecb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(\n",
    "    stop_words='english', # Removes a lot of common english words like it, and, that, is etc. Uses predifined scikit list of common english words.\n",
    "    sublinear_tf=True, # Uses logarithmic word frequency weighting, reducing the weight of extremely frequent terms & helps prevent domination by larger text files\n",
    "    max_features=10000, # Consideration for both overfitting and computational requirements.\n",
    "    ngram_range=(1,2)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0f18d575-9c5e-44f8-93dc-6c734ffebe80",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = vectorizer.fit_transform(train['text'])\n",
    "X_val = vectorizer.transform(val['text'])\n",
    "X_test = vectorizer.transform(test['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "186ef155-c63a-49e5-9318-4cbe9e847fed",
   "metadata": {},
   "source": [
    "Create seperate dataframe for TF-IDF Vectorization output. Next we'll want to append our stylometric features and our readability metrics to this dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cec915b3-0ab7-4ef7-9329-bf2018df6f2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1920, 10000)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9896f8ef-79a5-4991-8fe0-9a900ff388cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract stylometric features from a single text\n",
    "def extract_stylometric_features(text):\n",
    "    if not isinstance(text, str) or not text.strip():\n",
    "        return [0] * 12  # updated to match total number of features\n",
    "\n",
    "    # Use NLTK tokenizers with explicit language call\n",
    "    words = word_tokenize(text, language='english', preserve_line=True)\n",
    "    sentences = sent_tokenize(text, language='english')\n",
    "    chars = list(text)\n",
    "\n",
    "    word_lengths = [len(w) for w in words if w.isalpha()]\n",
    "    total_words = len(words)\n",
    "    unique_words = len(set(w.lower() for w in words if w.isalpha()))\n",
    "    total_sentences = len(sentences)\n",
    "\n",
    "    avg_word_length = np.mean(word_lengths) if word_lengths else 0\n",
    "    avg_sentence_length = total_words / total_sentences if total_sentences else 0\n",
    "    type_token_ratio = unique_words / total_words if total_words else 0\n",
    "\n",
    "    punctuation_counts = Counter(c for c in text if c in \".,!?;:-\")\n",
    "    punctuation_freqs = [punctuation_counts[p] / len(text) for p in \".,!?;:-\"]\n",
    "\n",
    "    uppercase_chars = sum(1 for c in text if c.isupper())\n",
    "    uppercase_ratio = uppercase_chars / len(text)\n",
    "\n",
    "    digit_ratio = sum(1 for c in text if c.isdigit()) / len(text)\n",
    "\n",
    "    return [\n",
    "        avg_word_length,\n",
    "        avg_sentence_length,\n",
    "        type_token_ratio,\n",
    "        *punctuation_freqs,\n",
    "        uppercase_ratio,\n",
    "        digit_ratio\n",
    "    ]\n",
    "\n",
    "# List of feature names\n",
    "stylo_feature_names = [\n",
    "    \"avg_word_length\",\n",
    "    \"avg_sentence_length\",\n",
    "    \"type_token_ratio\",\n",
    "    \"period_freq\", \"comma_freq\", \"exclam_freq\",\n",
    "    \"question_freq\", \"semicolon_freq\", \"colon_freq\", \"dash_freq\",\n",
    "    \"uppercase_ratio\", \"digit_ratio\"\n",
    "]\n",
    "\n",
    "# Apply to your dataset\n",
    "def get_stylometric_features(df):\n",
    "    features = df['text'].apply(extract_stylometric_features)\n",
    "    return pd.DataFrame(features.tolist(), columns=stylo_feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2f468ef0-d8f4-4673-8a72-75b98ef5227d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   avg_word_length  avg_sentence_length  type_token_ratio  period_freq  \\\n",
      "0         4.250333            15.664977          0.078897     0.010975   \n",
      "1         4.248404            14.571429          0.081391     0.013303   \n",
      "2         4.066894            20.021214          0.072704     0.012683   \n",
      "3         4.139240            20.450896          0.075695     0.012977   \n",
      "4         4.125742            21.993048          0.069692     0.012717   \n",
      "\n",
      "   comma_freq  exclam_freq  question_freq  semicolon_freq  colon_freq  \\\n",
      "0    0.012713     0.001262       0.001098        0.000138    0.000208   \n",
      "1    0.014442     0.001212       0.001377        0.000256    0.000176   \n",
      "2    0.013631     0.001044       0.001168        0.000244    0.000173   \n",
      "3    0.013200     0.001576       0.001345        0.000166    0.000083   \n",
      "4    0.015411     0.000932       0.001211        0.000102    0.000185   \n",
      "\n",
      "   dash_freq  uppercase_ratio  digit_ratio  \n",
      "0   0.002203         0.027425     0.000093  \n",
      "1   0.002391         0.030510     0.000172  \n",
      "2   0.001009         0.035503     0.000184  \n",
      "3   0.002146         0.028415     0.000000  \n",
      "4   0.001218         0.031257     0.000656  \n"
     ]
    }
   ],
   "source": [
    "# For train set\n",
    "train_stylo = get_stylometric_features(train)\n",
    "\n",
    "# For val and test\n",
    "val_stylo = get_stylometric_features(val)\n",
    "test_stylo = get_stylometric_features(test)\n",
    "\n",
    "# Preview\n",
    "print(train_stylo.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57cda95a-e08a-4f91-ad1d-5bf38e3c0da5",
   "metadata": {},
   "source": [
    "Add common readability quantification metrics to the dataset. Our training set will now be 1920 x (TF-IDF value + 12 + 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f1187045-d093-4fa8-b594-ae7932622b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def readability_metrics(text):\n",
    "    return [\n",
    "        textstat.flesch_reading_ease(text),\n",
    "        textstat.flesch_kincaid_grade(text),\n",
    "        textstat.gunning_fog(text),\n",
    "        textstat.smog_index(text),\n",
    "        textstat.coleman_liau_index(text),\n",
    "        textstat.automated_readability_index(text)\n",
    "    ]\n",
    "\n",
    "readability_feature_names = [\n",
    "    'flesch_reading_ease',\n",
    "    'flesch_kincaid_grade',\n",
    "    'gunning_fog',\n",
    "    'smog_index',\n",
    "    'coleman_liau_index',\n",
    "    'automated_readability_index'\n",
    "]\n",
    "\n",
    "# Apply to a DataFrame\n",
    "def get_readability_features(df):\n",
    "    scores = df['text'].apply(readability_metrics)\n",
    "    return pd.DataFrame(scores.tolist(), columns=readability_feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "73e6529b-0924-4695-ae75-5928ed8e29dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get readability feature matrices\n",
    "train_readability = get_readability_features(train)\n",
    "val_readability = get_readability_features(val)\n",
    "test_readability = get_readability_features(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "fe98090a-6101-4cc6-ab6b-4da3c865eee9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>flesch_reading_ease</th>\n",
       "      <th>flesch_kincaid_grade</th>\n",
       "      <th>gunning_fog</th>\n",
       "      <th>smog_index</th>\n",
       "      <th>coleman_liau_index</th>\n",
       "      <th>automated_readability_index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>73.78</td>\n",
       "      <td>6.5</td>\n",
       "      <td>6.68</td>\n",
       "      <td>9.1</td>\n",
       "      <td>7.71</td>\n",
       "      <td>7.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>75.40</td>\n",
       "      <td>5.9</td>\n",
       "      <td>6.20</td>\n",
       "      <td>8.6</td>\n",
       "      <td>7.29</td>\n",
       "      <td>6.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>74.90</td>\n",
       "      <td>6.1</td>\n",
       "      <td>6.16</td>\n",
       "      <td>8.4</td>\n",
       "      <td>6.77</td>\n",
       "      <td>6.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>75.40</td>\n",
       "      <td>5.9</td>\n",
       "      <td>6.00</td>\n",
       "      <td>8.6</td>\n",
       "      <td>6.83</td>\n",
       "      <td>6.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>74.59</td>\n",
       "      <td>6.2</td>\n",
       "      <td>6.24</td>\n",
       "      <td>8.3</td>\n",
       "      <td>7.30</td>\n",
       "      <td>7.1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   flesch_reading_ease  flesch_kincaid_grade  gunning_fog  smog_index  \\\n",
       "0                73.78                   6.5         6.68         9.1   \n",
       "1                75.40                   5.9         6.20         8.6   \n",
       "2                74.90                   6.1         6.16         8.4   \n",
       "3                75.40                   5.9         6.00         8.6   \n",
       "4                74.59                   6.2         6.24         8.3   \n",
       "\n",
       "   coleman_liau_index  automated_readability_index  \n",
       "0                7.71                          7.7  \n",
       "1                7.29                          6.9  \n",
       "2                6.77                          6.7  \n",
       "3                6.83                          6.6  \n",
       "4                7.30                          7.1  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_readability.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c34bc545-46bb-46e1-9175-29edff7045bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Horizontally concatenate\n",
    "train_extra = pd.concat([train_stylo, train_readability], axis=1)\n",
    "val_extra = pd.concat([val_stylo, val_readability], axis=1)\n",
    "test_extra = pd.concat([test_stylo, test_readability], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "325e97c1-4fa9-4b16-a208-ba9ac6edb1b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define labels from original dataset\n",
    "y_train = train['author']\n",
    "y_val = val['author']\n",
    "y_test = test['author']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "39299828-0019-4d2e-ba62-54a1c4d3f5ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, X_train, y_train, X_val, y_val):\n",
    "    # Train and predict\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_val)\n",
    "\n",
    "    # Metrics (set zero_division=0 to silence warnings)\n",
    "    acc = accuracy_score(y_val, y_pred)\n",
    "    f1 = f1_score(y_val, y_pred, average='weighted', zero_division=0)\n",
    "    precision = precision_score(y_val, y_pred, average='weighted', zero_division=0)\n",
    "    recall = recall_score(y_val, y_pred, average='weighted', zero_division=0)\n",
    "\n",
    "    # Print performance\n",
    "    print(f\"Model: {model.__class__.__name__}\")\n",
    "    print(f\"Accuracy:  {acc:.4f}\")\n",
    "    print(f\"F1 Score:  {f1:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall:    {recall:.4f}\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "    return model, y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "148e858e-c476-4615-b3b0-d7f655758b79",
   "metadata": {},
   "source": [
    "To run Naive Bayes we need only positive features, which our scaled stylometric and readability features do not adhere to. (TF-IDF features are strictly positive)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "63c6df1a-05ab-4546-bd0b-9cc89ebcbd9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive Bayes can't use standard scaler because it generated negative values, thus we pivot to min max scaler\n",
    "scaler = MinMaxScaler()\n",
    "train_scaled_nb = scaler.fit_transform(train_extra)\n",
    "val_scaled_nb = scaler.transform(val_extra)\n",
    "test_scaled_nb = scaler.transform(test_extra)\n",
    "\n",
    "X_train_combined_nb = hstack([X_train, csr_matrix(train_scaled_nb)])\n",
    "X_val_combined_nb   = hstack([X_val, csr_matrix(val_scaled_nb)])\n",
    "X_test_combined_nb  = hstack([X_test, csr_matrix(val_scaled_nb)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "468b3ba9-b738-469e-8e65-28fbfd4ff882",
   "metadata": {},
   "source": [
    "We can start with a baseline MultinomialNB model using our pre-defined max_features of 10,000 and n-gram setting of (1,2). Alpha will be kept as 1.0 for this first pass."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da9acc85-5704-4606-a161-6d4a21dfa49f",
   "metadata": {},
   "source": [
    "### Naive Bayes with multinomial NB & No stylometric features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8a83eebc-59f9-4b13-beaa-7592b671ef7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: MultinomialNB\n",
      "Accuracy:  0.8583\n",
      "F1 Score:  0.8483\n",
      "Precision: 0.8910\n",
      "Recall:    0.8583\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Naive Bayes, using unreduced feature set and stylo features\n",
    "nb_model_multi_no_stylo = MultinomialNB()\n",
    "model, y_pred = evaluate_model(nb_model_multi_no_stylo, X_train, y_train, X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "336d37b2-1864-4b8d-9d56-88f61498a266",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your text and labels\n",
    "X_train_texts = train['text']\n",
    "y_train = train['author']\n",
    "X_val_texts = val['text']\n",
    "y_val = val['author']\n",
    "\n",
    "# Parameter grid\n",
    "param_grid = {\n",
    "    'tfidf__max_features': [5000, 10000, 15000],\n",
    "    'tfidf__ngram_range': [(1, 1), (1, 2), (1, 3)],\n",
    "    'nb__alpha': [0.1, 0.5, 1.0, 2.0]\n",
    "}\n",
    "\n",
    "# Create all combinations of parameters\n",
    "param_combos = list(product(param_grid['tfidf__max_features'],\n",
    "                            param_grid['tfidf__ngram_range'],\n",
    "                            param_grid['nb__alpha']))\n",
    "\n",
    "results = []\n",
    "\n",
    "# Loop through each param combo\n",
    "for max_feat, ngram_range, alpha in param_combos:\n",
    "    pipeline = Pipeline([\n",
    "        ('tfidf', TfidfVectorizer(stop_words='english', sublinear_tf=True, max_features=max_feat, ngram_range=ngram_range)),\n",
    "        ('nb', MultinomialNB(alpha=alpha))\n",
    "    ])\n",
    "    \n",
    "    pipeline.fit(X_train_texts, y_train)\n",
    "    y_pred = pipeline.predict(X_val_texts)\n",
    "    \n",
    "    results.append({\n",
    "        'max_features': max_feat,\n",
    "        'ngram_range': ngram_range,\n",
    "        'alpha': alpha,\n",
    "        'accuracy': accuracy_score(y_val, y_pred),\n",
    "        'precision': precision_score(y_val, y_pred, average='weighted', zero_division=0),\n",
    "        'recall': recall_score(y_val, y_pred, average='weighted', zero_division=0),\n",
    "        'f1': f1_score(y_val, y_pred, average='weighted', zero_division=0)\n",
    "    })\n",
    "\n",
    "# Convert to DataFrame to view\n",
    "df_results = pd.DataFrame(results)\n",
    "df_results = df_results.sort_values(by='f1', ascending=False)\n",
    "\n",
    "# Convert to DataFrame\n",
    "df_results = pd.DataFrame(results)\n",
    "df_results = df_results.sort_values(by='f1', ascending=False)\n",
    "\n",
    "# Export to CSV\n",
    "df_results.to_csv('nb_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d4e2f34-e078-47de-8b34-8e06aae5b297",
   "metadata": {},
   "source": [
    "#### Now we can use the best set of parameters on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "6a403397-7104-4612-9884-df5c6ee7cbe2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Training Set Results:\n",
      "  Accuracy:  0.9792\n",
      "  F1 Score:  0.9792\n",
      "  Precision: 0.9807\n",
      "  Recall:    0.9792\n",
      "----------------------------------------\n",
      "ðŸ“Š Validation Set Results:\n",
      "  Accuracy:  0.9375\n",
      "  F1 Score:  0.9376\n",
      "  Precision: 0.9575\n",
      "  Recall:    0.9375\n",
      "----------------------------------------\n",
      "ðŸ“Š Test Set Results:\n",
      "  Accuracy:  0.9542\n",
      "  F1 Score:  0.9520\n",
      "  Precision: 0.9700\n",
      "  Recall:    0.9542\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Best parameters from grid search\n",
    "best_max_features = 15000\n",
    "best_ngram_range = (1, 3)\n",
    "best_alpha = 0.1\n",
    "\n",
    "# Text and labels\n",
    "X_train_texts = train['text']\n",
    "y_train = train['author']\n",
    "X_val_texts   = val['text']\n",
    "y_val   = val['author']\n",
    "X_test_texts  = test['text']\n",
    "y_test  = test['author']\n",
    "\n",
    "# Final pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(\n",
    "        stop_words='english',\n",
    "        sublinear_tf=True,\n",
    "        max_features=best_max_features,\n",
    "        ngram_range=best_ngram_range\n",
    "    )),\n",
    "    ('nb', MultinomialNB(alpha=best_alpha))\n",
    "])\n",
    "\n",
    "# Fit the model on training data\n",
    "pipeline.fit(X_train_texts, y_train)\n",
    "\n",
    "# Predict on all sets\n",
    "y_train_pred = pipeline.predict(X_train_texts)\n",
    "y_val_pred   = pipeline.predict(X_val_texts)\n",
    "y_test_pred  = pipeline.predict(X_test_texts)\n",
    "\n",
    "# Evaluation function\n",
    "def print_scores(name, y_true, y_pred):\n",
    "    print(f\"ðŸ“Š {name} Set Results:\")\n",
    "    print(f\"  Accuracy:  {accuracy_score(y_true, y_pred):.4f}\")\n",
    "    print(f\"  F1 Score:  {f1_score(y_true, y_pred, average='weighted', zero_division=0):.4f}\")\n",
    "    print(f\"  Precision: {precision_score(y_true, y_pred, average='weighted', zero_division=0):.4f}\")\n",
    "    print(f\"  Recall:    {recall_score(y_true, y_pred, average='weighted', zero_division=0):.4f}\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "# Print results\n",
    "print_scores(\"Training\", y_train, y_train_pred)\n",
    "print_scores(\"Validation\", y_val, y_val_pred)\n",
    "print_scores(\"Test\", y_test, y_test_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:gutenberg_env]",
   "language": "python",
   "name": "conda-env-gutenberg_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
